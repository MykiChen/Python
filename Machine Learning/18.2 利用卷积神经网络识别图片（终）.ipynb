{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用网络爬虫获取所需要训练的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36'}\n",
    "img_url = 'https://image.baidu.com/search/acjson?tn=resultjson_com&ipn=rj&ct=201326592&is=&fp=result&queryWord={0}&cl=2&lm=-1&ie=utf-8&oe=utf-8&adpicid=&st=-1&z=&ic=&hd=&latest=&copyright=&word={0}&s=&se=&tab=&width=&height=&face=0&istype=2&qc=&nc=1&fr=&expermode=&force=&cg=star&pn={1}&rn=30&gsm=&1578807000105='\n",
    "\n",
    "def getIdolPicture(keyword, dest_dir, batch):\n",
    "    # 建立图片储存路径\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "        \n",
    "    for i in range(batch):\n",
    "        res = requests.get(img_url.format(parse.quote(keyword), i*30))\n",
    "        res.encoding = 'utf-8'\n",
    "        print(len(res.json()['data']))\n",
    "        for ele in res.json()['data']:\n",
    "            url = ele.get('thumbURL')\n",
    "            if url:\n",
    "                # 获取图片链接最后部分并做为图片名字\n",
    "                with open(dest_dir + url.split('/')[-1], 'wb') as f:\n",
    "                    res2 = requests.get(url, headers=headers)\n",
    "                    f.write(res2.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "getIdolPicture('范冰冰', 'idol1/', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "getIdolPicture('迪丽热巴', 'idol2/', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "getIdolPicture('林志玲', 'idol3/', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用OpenCV 撷取人脸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv.CascadeClassifier('/usr/local/Cellar/opencv/4.2.0_1/share/opencv4/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def extractFace(filename, src_dir, dest_dir):\n",
    "    img = cv.imread(src_dir + filename)\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    im = Image.open(src_dir + filename)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        box = (x, y, x+w, y+h)\n",
    "        crpim = im.crop(box).resize((64, 64))\n",
    "        crpim.save(dest_dir + filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u=4252810034,3548750092&fm=11&gp=0.jpg\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('idol1_face'):\n",
    "    os.mkdir('idol1_face')\n",
    "\n",
    "for f in os.listdir('idol1/'):\n",
    "    try:\n",
    "        extractFace(f, 'idol1/', 'idol1_face/')\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u=2131554868,2623628550&fm=26&gp=0.jpg\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('idol2_face'):\n",
    "    os.mkdir('idol2_face')\n",
    "\n",
    "for f in os.listdir('idol2/'):\n",
    "    try:\n",
    "        extractFace(f, 'idol2/', 'idol2_face/')\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u=1980474683,3378689882&fm=26&gp=0.jpg\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('idol3_face'):\n",
    "    os.mkdir('idol3_face')\n",
    "\n",
    "for f in os.listdir('idol3'):\n",
    "    try:\n",
    "        extractFace(f, 'idol3/', 'idol3_face/')\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用卷积神经网络识别明星图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建构卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "\n",
    "# Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Fully Connected\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam',\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像增广技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1. / 255,\n",
    "                                  shear_range = 0.2,    # x 坐标保持不变， 而对应y 的坐标按比例发生平移\n",
    "                                  zoom_range = 0.2,     # 可以让图片在长或宽的方向进行放大\n",
    "                                  horizontal_flip = True  # 水平翻转操作\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1. / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立训练与测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    'trainset/', target_size=(64, 64),\n",
    "    batch_size=30, \n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "    'testset/', target_size=(64, 64),\n",
    "    batch_size=30,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 1.1266 - accuracy: 0.3796 - val_loss: 1.0812 - val_accuracy: 0.3380\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 82ms/step - loss: 1.0720 - accuracy: 0.4163 - val_loss: 1.0172 - val_accuracy: 0.4085\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 1.0136 - accuracy: 0.4878 - val_loss: 0.8654 - val_accuracy: 0.5634\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.9834 - accuracy: 0.5286 - val_loss: 0.8895 - val_accuracy: 0.6479\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 0.9135 - accuracy: 0.5878 - val_loss: 0.8661 - val_accuracy: 0.5775\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 1s 80ms/step - loss: 0.8971 - accuracy: 0.5796 - val_loss: 0.8937 - val_accuracy: 0.6338\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.8039 - accuracy: 0.6367 - val_loss: 0.8423 - val_accuracy: 0.6620\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 1s 80ms/step - loss: 0.8097 - accuracy: 0.6245 - val_loss: 0.5737 - val_accuracy: 0.7465\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 1s 82ms/step - loss: 0.8024 - accuracy: 0.6408 - val_loss: 0.8850 - val_accuracy: 0.6197\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.7891 - accuracy: 0.6061 - val_loss: 0.5009 - val_accuracy: 0.7183\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.7666 - accuracy: 0.6653 - val_loss: 0.9440 - val_accuracy: 0.7183\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 1s 82ms/step - loss: 0.6985 - accuracy: 0.7082 - val_loss: 0.9109 - val_accuracy: 0.7606\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.7621 - accuracy: 0.6714 - val_loss: 0.6920 - val_accuracy: 0.5493\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.7273 - accuracy: 0.6918 - val_loss: 0.6356 - val_accuracy: 0.7465\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.6989 - accuracy: 0.7041 - val_loss: 0.3962 - val_accuracy: 0.7746\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 1s 82ms/step - loss: 0.6107 - accuracy: 0.7510 - val_loss: 0.6201 - val_accuracy: 0.7887\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.5878 - accuracy: 0.7653 - val_loss: 0.6473 - val_accuracy: 0.8169\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 1s 82ms/step - loss: 0.5796 - accuracy: 0.7653 - val_loss: 0.4895 - val_accuracy: 0.8028\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.5719 - accuracy: 0.7776 - val_loss: 0.6770 - val_accuracy: 0.7183\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.6372 - accuracy: 0.7388 - val_loss: 0.6320 - val_accuracy: 0.7324\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.6228 - accuracy: 0.7469 - val_loss: 1.0548 - val_accuracy: 0.7746\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.5248 - accuracy: 0.7776 - val_loss: 0.7656 - val_accuracy: 0.7887\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.4610 - accuracy: 0.8061 - val_loss: 0.2977 - val_accuracy: 0.8028\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.5157 - accuracy: 0.7755 - val_loss: 0.2520 - val_accuracy: 0.8169\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.4373 - accuracy: 0.8327 - val_loss: 1.0893 - val_accuracy: 0.8592\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.5200 - accuracy: 0.7980 - val_loss: 0.2654 - val_accuracy: 0.8028\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.4391 - accuracy: 0.8265 - val_loss: 0.4374 - val_accuracy: 0.8028\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.4348 - accuracy: 0.8082 - val_loss: 0.4965 - val_accuracy: 0.8028\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.4357 - accuracy: 0.8347 - val_loss: 0.7760 - val_accuracy: 0.7746\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.4766 - accuracy: 0.8265 - val_loss: 0.2466 - val_accuracy: 0.8169\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.3813 - accuracy: 0.8429 - val_loss: 0.9710 - val_accuracy: 0.8028\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 1s 79ms/step - loss: 0.3755 - accuracy: 0.8653 - val_loss: 0.9220 - val_accuracy: 0.8451\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.3651 - accuracy: 0.8490 - val_loss: 0.8153 - val_accuracy: 0.8310\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.4172 - accuracy: 0.8388 - val_loss: 0.3811 - val_accuracy: 0.8451\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.4019 - accuracy: 0.8449 - val_loss: 0.2293 - val_accuracy: 0.8169\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.3163 - accuracy: 0.8816 - val_loss: 0.7935 - val_accuracy: 0.8451\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.3207 - accuracy: 0.8653 - val_loss: 0.0970 - val_accuracy: 0.8451\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.3312 - accuracy: 0.8918 - val_loss: 0.1194 - val_accuracy: 0.8310\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 2s 97ms/step - loss: 0.3041 - accuracy: 0.8980 - val_loss: 0.6754 - val_accuracy: 0.8451\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.3333 - accuracy: 0.8714 - val_loss: 0.0252 - val_accuracy: 0.8451\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.2681 - accuracy: 0.9020 - val_loss: 0.9377 - val_accuracy: 0.8169\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.2688 - accuracy: 0.9000 - val_loss: 0.4737 - val_accuracy: 0.8451\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.2674 - accuracy: 0.8939 - val_loss: 0.2921 - val_accuracy: 0.8451\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.2287 - accuracy: 0.9020 - val_loss: 0.3194 - val_accuracy: 0.8451\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.2460 - accuracy: 0.9143 - val_loss: 1.5438 - val_accuracy: 0.8451\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.2901 - accuracy: 0.8918 - val_loss: 0.6947 - val_accuracy: 0.8169\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.2286 - accuracy: 0.9184 - val_loss: 1.8970 - val_accuracy: 0.8592\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 2s 98ms/step - loss: 0.2501 - accuracy: 0.8918 - val_loss: 0.5334 - val_accuracy: 0.8732\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 2s 98ms/step - loss: 0.2445 - accuracy: 0.9020 - val_loss: 0.7514 - val_accuracy: 0.8451\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.2645 - accuracy: 0.9000 - val_loss: 0.4907 - val_accuracy: 0.8732\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.2105 - accuracy: 0.9245 - val_loss: 0.0390 - val_accuracy: 0.9155\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.2300 - accuracy: 0.9184 - val_loss: 0.7151 - val_accuracy: 0.8169\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.2847 - accuracy: 0.8898 - val_loss: 0.1794 - val_accuracy: 0.8873\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.2515 - accuracy: 0.8980 - val_loss: 0.3701 - val_accuracy: 0.8451\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.2072 - accuracy: 0.9327 - val_loss: 0.3309 - val_accuracy: 0.8028\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.2006 - accuracy: 0.9286 - val_loss: 0.3503 - val_accuracy: 0.8592\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.2168 - accuracy: 0.9184 - val_loss: 0.0138 - val_accuracy: 0.8732\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1583 - accuracy: 0.9408 - val_loss: 0.3342 - val_accuracy: 0.8169\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1515 - accuracy: 0.9449 - val_loss: 1.2476 - val_accuracy: 0.8451\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.2054 - accuracy: 0.9224 - val_loss: 0.6892 - val_accuracy: 0.8732\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1937 - accuracy: 0.9306 - val_loss: 0.4029 - val_accuracy: 0.8873\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.1583 - accuracy: 0.9408 - val_loss: 0.6433 - val_accuracy: 0.8451\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1502 - accuracy: 0.9490 - val_loss: 0.7011 - val_accuracy: 0.8732\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.1676 - accuracy: 0.9367 - val_loss: 0.4370 - val_accuracy: 0.8310\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.1279 - accuracy: 0.9490 - val_loss: 0.4639 - val_accuracy: 0.8169\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1583 - accuracy: 0.9510 - val_loss: 0.0237 - val_accuracy: 0.8451\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.1498 - accuracy: 0.9469 - val_loss: 1.4492 - val_accuracy: 0.8028\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1035 - accuracy: 0.9673 - val_loss: 1.1867 - val_accuracy: 0.8592\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.1318 - accuracy: 0.9531 - val_loss: 0.2688 - val_accuracy: 0.8592\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 2s 88ms/step - loss: 0.1410 - accuracy: 0.9449 - val_loss: 0.1208 - val_accuracy: 0.8310\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1256 - accuracy: 0.9673 - val_loss: 0.1922 - val_accuracy: 0.8732\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1126 - accuracy: 0.9571 - val_loss: 1.0385 - val_accuracy: 0.9014\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1336 - accuracy: 0.9551 - val_loss: 0.3994 - val_accuracy: 0.9014\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.1212 - accuracy: 0.9551 - val_loss: 0.8496 - val_accuracy: 0.8028\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.1676 - accuracy: 0.9408 - val_loss: 0.3175 - val_accuracy: 0.8732\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.1374 - accuracy: 0.9449 - val_loss: 0.6283 - val_accuracy: 0.8873\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1321 - accuracy: 0.9551 - val_loss: 0.7658 - val_accuracy: 0.8732\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1234 - accuracy: 0.9510 - val_loss: 0.7904 - val_accuracy: 0.8592\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.0984 - accuracy: 0.9735 - val_loss: 0.4582 - val_accuracy: 0.8310\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.0832 - accuracy: 0.9673 - val_loss: 0.1899 - val_accuracy: 0.8451\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1174 - accuracy: 0.9592 - val_loss: 0.0621 - val_accuracy: 0.8028\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.1029 - accuracy: 0.9592 - val_loss: 1.7970 - val_accuracy: 0.8592\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1190 - accuracy: 0.9490 - val_loss: 1.9980 - val_accuracy: 0.8732\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.2042 - accuracy: 0.9204 - val_loss: 1.8388 - val_accuracy: 0.8028\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 1s 83ms/step - loss: 0.2698 - accuracy: 0.8980 - val_loss: 0.9528 - val_accuracy: 0.8169\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1859 - accuracy: 0.9347 - val_loss: 0.2605 - val_accuracy: 0.8592\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.1564 - accuracy: 0.9367 - val_loss: 0.0314 - val_accuracy: 0.8873\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1844 - accuracy: 0.9204 - val_loss: 1.2624 - val_accuracy: 0.8592\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1009 - accuracy: 0.9755 - val_loss: 1.5592 - val_accuracy: 0.7746\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 1s 88ms/step - loss: 0.0938 - accuracy: 0.9673 - val_loss: 0.2541 - val_accuracy: 0.8310\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.0909 - accuracy: 0.9776 - val_loss: 2.0645 - val_accuracy: 0.8873\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.0665 - accuracy: 0.9735 - val_loss: 0.5155 - val_accuracy: 0.8310\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.0715 - accuracy: 0.9735 - val_loss: 0.4047 - val_accuracy: 0.8451\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.1200 - accuracy: 0.9551 - val_loss: 1.0965 - val_accuracy: 0.8169\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.0843 - accuracy: 0.9653 - val_loss: 0.0256 - val_accuracy: 0.9014\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.0781 - accuracy: 0.9735 - val_loss: 0.0328 - val_accuracy: 0.9014\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 1s 81ms/step - loss: 0.0704 - accuracy: 0.9776 - val_loss: 0.0139 - val_accuracy: 0.9014\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.0690 - accuracy: 0.9776 - val_loss: 0.2841 - val_accuracy: 0.8873\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.0900 - accuracy: 0.9633 - val_loss: 0.4074 - val_accuracy: 0.8451\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 0.0656 - accuracy: 0.9816 - val_loss: 2.0690 - val_accuracy: 0.8310\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(\n",
    "    training_set, \n",
    "    epochs = 100,\n",
    "    verbose=1,\n",
    "    validation_data= test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测单张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "## 截取图片的面部特征\n",
    "face_cascade = cv.CascadeClassifier('/usr/local/Cellar/opencv/4.2.0_1/share/opencv4/haarcascades/haarcascade_frontalface_default.xml')\n",
    "img = cv.imread('timg.jpeg')\n",
    "faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "im = Image.open('timg.jpeg')\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    box = (x, y, x+w, y+h)\n",
    "    crpim = im.crop(box).resize((64, 64))\n",
    "    crpim.save('test3.jpeg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('prediction_dataset/i1_d.jpg', target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "classifier.predict_classes(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand_dims(a, axis)就是在axis的那一个轴上把数据加上去，这个数据在axis这个轴的0位置。\n",
    "\n",
    "例如原本为一维的2个数据，axis=0，则shape变为(1,2),axis=1则shape变为(2,1)\n",
    "\n",
    "再例如 原本为 (2,3),axis=0，则shape变为(1,2,3),axis=1则shape变为(2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片：范冰冰3.jpg 是范冰冰\n",
      "图片：范冰冰2.jpg 是范冰冰\n",
      "图片：林志玲4.jpeg 是林志玲\n",
      "图片：林志玲3.jpg 是林志玲\n",
      "图片：林志玲2.jpg 是林志玲\n",
      "图片：林志玲.jpg 是林志玲\n",
      "图片：迪丽热巴3.jpeg 是迪丽热巴\n",
      "图片：迪丽热巴.jpeg 是迪丽热巴\n",
      "图片：范冰冰.jpg 是范冰冰\n",
      "图片：迪丽热巴2.jpg 是林志玲\n",
      "图片：迪丽热巴3.jpg 是迪丽热巴\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir('prediction_dataset/'):\n",
    "    if f != '.DS_Store':\n",
    "        test_image = image.load_img('prediction_dataset/{}'.format(f), target_size=(64, 64))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        num = classifier.predict_classes(test_image)\n",
    "        if num == 0:\n",
    "            print('图片：' + f + ' 是范冰冰')\n",
    "        elif num == 1:\n",
    "            print('图片：' + f + ' 是迪丽热巴')\n",
    "        elif num == 2:\n",
    "            print('图片：' + f + ' 是林志玲')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
